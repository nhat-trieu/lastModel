# ==========================================
# B∆Ø·ªöC 2.5: TR√çCH XU·∫§T OBJECTS + PRIORS (NPZ + BOXES + ZIP)
# ==========================================
import os
import torch
import torch.nn as nn
import numpy as np
from PIL import Image
import cv2
import glob
from tqdm.auto import tqdm
import warnings
from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights
from torchvision import transforms
import timm
import shutil

warnings.filterwarnings('ignore')

# 1. C·∫•u h√¨nh
CONFIG = {
    'data_root': '/kaggle/input/groupemowfull/GroupEmoW',
    'base_output': '/kaggle/working/features_congnn',
    'output_dir': '/kaggle/working/features_congnn/objects',
    'max_objects': 10, # C√≥ th·ªÉ tƒÉng th√™m n·∫øu mu·ªën l·∫•y nhi·ªÅu v·∫≠t th·ªÉ h∆°n
    'conf_threshold': 0.5,
    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')
}

os.makedirs(CONFIG['output_dir'], exist_ok=True)

# 2. Chu·∫©n b·ªã Model
print("üöÄ ƒêang t·∫£i model Object Detection & Feature Extraction...")
# Detector l·∫•y t·ª´ torchvision (Faster R-CNN)
detector = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT).to(CONFIG['device'])
detector.eval()

# Feature Extractor (SE-ResNet50)
feature_extractor = timm.create_model('seresnet50', pretrained=True, num_classes=0).to(CONFIG['device'])
feature_extractor.eval()

obj_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

def find_emotion_data_path(root_path, split_keyword):
    for root, dirs, files in os.walk(root_path):
        if split_keyword.lower() in os.path.basename(root).lower():
            lower_dirs = [d.lower() for d in dirs]
            if 'positive' in lower_dirs and 'negative' in lower_dirs:
                return root
    return None

# 3. Main Loop
def run_object_extraction_with_priors():
    print(f"üîß Device: {CONFIG['device']}")
    splits = ['train', 'val', 'test']
    emotions = ['Negative', 'Neutral', 'Positive']
    
    for split in splits:
        real_path = find_emotion_data_path(CONFIG['data_root'], split)
        if not real_path: continue
        
        print(f"\nüì¶ ƒêang x·ª≠ l√Ω OBJECTS t·∫≠p {split.upper()}...")
        
        for emotion in emotions:
            emo_dir = next((os.path.join(real_path, d) for d in os.listdir(real_path) 
                          if d.lower() == emotion.lower()), None)
            if not emo_dir: continue
            
            save_path = os.path.join(CONFIG['output_dir'], split, emotion.lower())
            os.makedirs(save_path, exist_ok=True)
            
            img_files = glob.glob(os.path.join(emo_dir, '*'))
            img_files = [f for f in img_files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
            
            for img_path in tqdm(img_files, desc=f" {emotion}", leave=False):
                base = os.path.splitext(os.path.basename(img_path))[0]
                dst_file = os.path.join(save_path, base + '.npz')
                
                if os.path.exists(dst_file): continue
                
                try:
                    img_pil = Image.open(img_path).convert('RGB')
                    width, height = img_pil.size
                    
                    # Ph√°t hi·ªán v·∫≠t th·ªÉ
                    img_tensor = transforms.ToTensor()(img_pil).to(CONFIG['device'])
                    with torch.no_grad():
                        prediction = detector([img_tensor])[0]
                    
                    boxes = prediction['boxes']
                    scores = prediction['scores']
                    
                    # L·ªçc theo ng∆∞·ª°ng tin c·∫≠y
                    keep = scores > CONFIG['conf_threshold']
                    boxes = boxes[keep]
                    scores = scores[keep]
                    
                    # Ch·ªâ l·∫•y t·ªëi ƒëa N v·∫≠t th·ªÉ to nh·∫•t/quan tr·ªçng nh·∫•t
                    if len(boxes) > CONFIG['max_objects']:
                        boxes = boxes[:CONFIG['max_objects']]
                        scores = scores[:CONFIG['max_objects']]

                    valid_feats = []
                    valid_boxes = []
                    valid_scores = []
                    valid_areas = []

                    if len(boxes) > 0:
                        batch_crops = []
                        for i, box in enumerate(boxes):
                            x1, y1, x2, y2 = box.tolist()
                            x1, y1, x2, y2 = max(0, int(x1)), max(0, int(y1)), min(width, int(x2)), min(height, int(y2))
                            
                            if x2 > x1 and y2 > y1:
                                crop = img_pil.crop((x1, y1, x2, y2))
                                batch_crops.append(obj_transform(crop))
                                
                                # T√≠nh di·ªán t√≠ch v·∫≠t th·ªÉ
                                area = (x2 - x1) * (y2 - y1)
                                valid_boxes.append([x1, y1, x2, y2])
                                valid_scores.append(scores[i].item())
                                valid_areas.append(area)
                        
                        if batch_crops:
                            # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng h√†ng lo·∫°t
                            batch_tensor = torch.stack(batch_crops).to(CONFIG['device'])
                            with torch.no_grad():
                                # SE-ResNet50 tr√≠ch xu·∫•t ra vector (num_objs, 2048)
                                feats = feature_extractor(batch_tensor)
                                valid_feats = feats.cpu().numpy()

                            # T√≠nh to√°n Priors cho Object
                            valid_areas = np.array(valid_areas, dtype=np.float32)
                            valid_scores = np.array(valid_scores, dtype=np.float32)
                            priors = valid_areas * valid_scores
                            
                            # Chu·∫©n h√≥a Prior v·ªÅ [0, 1]
                            if priors.max() > 0:
                                priors = priors / (priors.max() + 1e-9)
                            
                            np.savez_compressed(dst_file, 
                                features=valid_feats.astype(np.float32),
                                boxes=np.array(valid_boxes, dtype=np.float32),
                                confidences=valid_scores.astype(np.float32),
                                areas=valid_areas,
                                priors=priors.astype(np.float32))
                        else:
                            # Kh√¥ng c√≥ crop n√†o h·ª£p l·ªá
                            self_save_empty(dst_file)
                    else:
                        # Kh√¥ng t√¨m th·∫•y v·∫≠t th·ªÉ
                        self_save_empty(dst_file)
                except Exception as e:
                    # print(f"L·ªói: {e}")
                    continue

    # 4. N√©n folder k·∫øt qu·∫£
    print("\nüì¶ ƒêang n√©n folder OBJECTS...")
    zip_path = '/kaggle/working/objects_features_priors'
    shutil.make_archive(zip_path, 'zip', 
                        root_dir=CONFIG['base_output'], 
                        base_dir='objects')
    print(f"‚úÖ Xong! File n√©n t·∫°i: {zip_path}.zip")

def self_save_empty(dst_file):
    """H√†m ph·ª• ƒë·ªÉ l∆∞u file r·ªóng khi kh√¥ng detect ƒë∆∞·ª£c g√¨"""
    np.savez_compressed(dst_file, 
        features=np.zeros((0, 2048), dtype=np.float32),
        boxes=np.zeros((0, 4), dtype=np.float32),
        confidences=np.zeros(0, dtype=np.float32),
        areas=np.zeros(0, dtype=np.float32),
        priors=np.zeros(0, dtype=np.float32))

if __name__ == "__main__":
    run_object_extraction_with_priors()