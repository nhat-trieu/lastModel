import os
import torch
import torch.nn as nn
import numpy as np
from PIL import Image
import glob
from tqdm.auto import tqdm
from torchvision import transforms
import timm 
import shutil
import warnings

warnings.filterwarnings('ignore')

# ==========================================
# 1. C·∫§U H√åNH (CONFIG)
# ==========================================
CONFIG = {
    'data_root': '/kaggle/input/groupemowfull', 
    'scene_weights': '/kaggle/input/resnet50-scene-combined/resnet50_scene_combined.pth', 
    'output_dir': '/kaggle/working/features_congnn/scenes', # Th∆∞ m·ª•c ƒë·ªìng b·ªô v·ªõi c√°c ph·∫ßn kh√°c
    'zip_name': '/kaggle/working/scene_features_final',
    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')
}

os.makedirs(CONFIG['output_dir'], exist_ok=True)

# ==========================================
# 2. H√ÄM KH·ªûI T·∫†O MODEL SE-RESNET50 (CHU·∫®N IPR-MPNN)
# ==========================================
def get_congnn_scene_extractor():
    print(f"üöÄ Kh·ªüi t·∫°o SE-ResNet50 Extractor...")
    
    # Kh·ªüi t·∫°o model seresnet50 (Squeeze-and-Excitation gi√∫p focus b·ªëi c·∫£nh t·ªët h∆°n)
    model = timm.create_model('seresnet50', pretrained=False, num_classes=3)
    
    # Load weights custom c·ªßa √¥ng
    if os.path.exists(CONFIG['scene_weights']):
        print(f"‚úÖ ƒêang n·∫°p weights t·ª´: {CONFIG['scene_weights']}")
        checkpoint = torch.load(CONFIG['scene_weights'], map_location=CONFIG['device'])
        state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint
        
        try:
            model.load_state_dict(state_dict)
            print("  ‚ú® N·∫°p weights th√†nh c√¥ng!")
        except RuntimeError:
            print(f"  ‚ö†Ô∏è ƒêang n·∫°p weights v·ªõi strict=False...")
            model.load_state_dict(state_dict, strict=False)
    else:
        print("  ‚ùå Kh√¥ng th·∫•y weights. S·ª≠ d·ª•ng ImageNet m·∫∑c ƒë·ªãnh.")
        model = timm.create_model('seresnet50', pretrained=True, num_classes=3)

    # X√≥a l·ªõp ph√¢n lo·∫°i ƒë·ªÉ l·∫•y feature vector 2048 chi·ªÅu
    model.reset_classifier(num_classes=0) 
    model = model.to(CONFIG['device'])
    model.eval()
    return model

# ==========================================
# 3. TH·ª∞C THI TR√çCH XU·∫§T (SAVE NPZ + PRIORS)
# ==========================================
def run_scene_extraction():
    extractor = get_congnn_scene_extractor()
    
    # Transform chu·∫©n c·ªßa SE-ResNet
    scene_tf = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    splits = ['train', 'val', 'test']
    emotions = ['Negative', 'Neutral', 'Positive']
    
    for split in splits:
        split_path = None
        for root, dirs, files in os.walk(CONFIG['data_root']):
            if split.lower() == os.path.basename(root).lower():
                split_path = root
                break
        
        if not split_path: continue
            
        print(f"\nüìÇ ƒêang x·ª≠ l√Ω SCENE t·∫≠p {split.upper()}...")
        out_root = os.path.join(CONFIG['output_dir'], split)
        
        for emotion in emotions:
            emo_dir = next((os.path.join(split_path, d) for d in os.listdir(split_path) 
                          if d.lower() == emotion.lower()), None)
            if not emo_dir: continue
            
            save_path = os.path.join(out_root, emotion.lower())
            os.makedirs(save_path, exist_ok=True)
            
            img_files = glob.glob(os.path.join(emo_dir, '*'))
            img_files = [f for f in img_files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            
            for img_path in tqdm(img_files, desc=f" {emotion}", leave=False):
                base = os.path.splitext(os.path.basename(img_path))[0]
                dst_path = os.path.join(save_path, base + '.npz') # CHUY·ªÇN SANG .NPZ
                
                if os.path.exists(dst_path): continue
                
                try:
                    img_pil = Image.open(img_path).convert('RGB')
                    width, height = img_pil.size
                    
                    # A. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng h√¨nh ·∫£nh
                    img_tensor = scene_tf(img_pil).unsqueeze(0).to(CONFIG['device'])
                    with torch.no_grad():
                        feat = extractor(img_tensor)
                        feat_np = feat.cpu().numpy() # Shape (1, 2048)
                    
                    # B. Thi·∫øt l·∫≠p th√¥ng tin C·∫•u tr√∫c (Prior) cho Scene node
                    # Box l√† to√†n b·ªô ·∫£nh: [x1, y1, x2, y2]
                    box = np.array([[0, 0, width, height]], dtype=np.float32)
                    # ƒê·ªô tin c·∫≠y cho Scene node m·∫∑c ƒë·ªãnh l√† 1.0
                    conf = np.array([1.0], dtype=np.float32)
                    # Di·ªán t√≠ch to√†n ·∫£nh
                    area = np.array([width * height], dtype=np.float32)
                    # Prior c·ª±c k·ª≥ quan tr·ªçng cho IPR-MPNN: Scene lu√¥n l√† 1.0
                    prior = np.array([1.0], dtype=np.float32)
                    
                    # C. L∆∞u NPZ n√©n (ƒê·ªìng b·ªô v·ªõi Face v√† Object)
                    np.savez_compressed(dst_path, 
                        features=feat_np.astype(np.float32),
                        boxes=box,
                        confidences=conf,
                        areas=area,
                        priors=prior
                    )
                except Exception: 
                    continue

    # ==========================================
    # 4. N√âN K·∫æT QU·∫¢
    # ==========================================
    print("\nüì¶ ƒêang n√©n k·∫øt qu·∫£...")
    # N√©n folder ch·ª©a c√°c folder con train/val/test
    shutil.make_archive(CONFIG['zip_name'], 'zip', CONFIG['output_dir'])
    print(f"‚úÖ Ho√†n t·∫•t! File l∆∞u t·∫°i: {CONFIG['zip_name']}.zip")

if __name__ == "__main__":
    run_scene_extraction()