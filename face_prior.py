import os
import torch
import torch.nn as nn
import numpy as np
from PIL import Image
import glob
from tqdm.auto import tqdm
from torchvision import models, transforms
from facenet_pytorch import MTCNN 
import shutil
import warnings

warnings.filterwarnings('ignore')

# ==========================================
# 1. C·∫§U H√åNH (CONFIG)
# ==========================================
CONFIG = {
    'data_root': '/kaggle/input/groupemowfull',
    'face_weights': '/kaggle/input/resnet50-face-best-112-v2/resnet50_face_best_112_v2.pth',
    'output_dir': '/kaggle/working/features_congnn_priors',
    'min_conf': 0.8,
    'min_face_size': 30,
    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')
}

os.makedirs(CONFIG['output_dir'], exist_ok=True)

# ==========================================
# 2. KH·ªûI T·∫†O MODEL (GI·ªÆ KI·∫æN TR√öC THEO WEIGHTS 112x112)
# ==========================================
def build_resnet50_extractor(weights_path):
    # Ki·∫øn tr√∫c n√†y d√†nh ri√™ng cho weights face fine-tuned c·ªßa √¥ng
    model = models.resnet50(pretrained=False)
    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
    model.maxpool = nn.Identity()
    model.fc = nn.Linear(model.fc.in_features, 3) # Gi·∫£ ƒë·ªãnh weights c≈© c√≥ l·ªõp cu·ªëi l√† 3 emotions
    
    if os.path.exists(weights_path):
        print(f"‚úÖ ƒêang n·∫°p tr·ªçng s·ªë Face fine-tuned t·ª´: {weights_path}")
        model.load_state_dict(torch.load(weights_path, map_location=CONFIG['device']))
    else:
        print("‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng th·∫•y file weights! Model s·∫Ω ch·∫°y v·ªõi random weights.")

    # B·ªè l·ªõp FC cu·ªëi ƒë·ªÉ l·∫•y 2048 ƒë·∫∑c tr∆∞ng (Global Average Pooling output)
    extractor = nn.Sequential(*list(model.children())[:-1])
    extractor = extractor.to(CONFIG['device'])
    extractor.eval()
    return extractor

# ==========================================
# 3. QUY TR√åNH TR√çCH XU·∫§T (ALL FACES + PRIORS)
# ==========================================
def run_extraction_full_priors():
    # Kh·ªüi t·∫°o MTCNN l·∫•y to√†n b·ªô m·∫∑t
    mtcnn = MTCNN(
        keep_all=True, 
        device=CONFIG['device'], 
        min_face_size=CONFIG['min_face_size'],
        post_process=False
    )
    
    face_extractor = build_resnet50_extractor(CONFIG['face_weights'])
    
    # Image Net normalization (th√¥ng s·ªë chu·∫©n cho ResNet)
    face_tf = transforms.Compose([
        transforms.Resize((112, 112)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    splits = ['train', 'val', 'test']
    emotions = ['Negative', 'Neutral', 'Positive']

    for split in splits:
        # T√¨m ƒë∆∞·ªùng d·∫´n split (kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng)
        split_path = None
        for root, dirs, files in os.walk(CONFIG['data_root']):
            if split.lower() == os.path.basename(root).lower():
                split_path = root
                break
        if not split_path: continue

        print(f"\nüë§ ƒêang x·ª≠ l√Ω FACES t·∫≠p {split.upper()}...")
        
        for emotion in emotions:
            emo_dir = os.path.join(split_path, emotion)
            if not os.path.exists(emo_dir): continue
            
            save_path = os.path.join(CONFIG['output_dir'], split, emotion.lower())
            os.makedirs(save_path, exist_ok=True)
            
            img_files = glob.glob(os.path.join(emo_dir, '*'))
            img_files = [f for f in img_files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]

            for img_path in tqdm(img_files, desc=f" {emotion}", leave=False):
                base = os.path.splitext(os.path.basename(img_path))[0]
                dst_file = os.path.join(save_path, base + '.npz')
                
                if os.path.exists(dst_file): continue

                try:
                    img_pil = Image.open(img_path).convert('RGB')
                    
                    # 1. Ph√°t hi·ªán m·∫∑t v√† l·∫•y x√°c su·∫•t
                    boxes, probs = mtcnn.detect(img_pil)
                    
                    if boxes is not None:
                        # L·ªçc theo threshold
                        mask = (probs > CONFIG['min_conf'])
                        boxes = boxes[mask]
                        probs = probs[mask]
                        
                        if len(boxes) > 0:
                            # T√≠nh di·ªán t√≠ch
                            areas = (boxes[:,2]-boxes[:,0]) * (boxes[:,3]-boxes[:,1])
                            
                            # S·∫Øp x·∫øp m·∫∑t to l√™n tr∆∞·ªõc
                            sort_idx = np.argsort(areas)[::-1]
                            boxes = boxes[sort_idx]
                            probs = probs[sort_idx]
                            areas = areas[sort_idx]
                            
                            face_batch = []
                            valid_boxes = []
                            valid_probs = []
                            valid_areas = []
                            
                            for i, box in enumerate(boxes):
                                x1, y1, x2, y2 = [int(b) for b in box]
                                # Crop an to√†n
                                x1, y1 = max(0, x1), max(0, y1)
                                x2, y2 = min(img_pil.width, x2), min(img_pil.height, y2)
                                
                                if x2 > x1 and y2 > y1:
                                    face_crop = img_pil.crop((x1, y1, x2, y2))
                                    face_batch.append(face_tf(face_crop))
                                    valid_boxes.append([x1, y1, x2, y2])
                                    valid_probs.append(probs[i])
                                    valid_areas.append(areas[i])
                            
                            if len(face_batch) > 0:
                                # 2. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
                                with torch.no_grad():
                                    batch_tensor = torch.stack(face_batch).to(CONFIG['device'])
                                    feats = face_extractor(batch_tensor)
                                    f_feats = torch.flatten(feats, 1).cpu().numpy()
                                
                                # 3. T√≠nh Priors (Area * Confidence)
                                valid_areas = np.array(valid_areas, dtype=np.float32)
                                valid_probs = np.array(valid_probs, dtype=np.float32)
                                priors = valid_areas * valid_probs
                                
                                # Chu·∫©n h√≥a priors v·ªÅ [0, 1] trong n·ªôi b·ªô ·∫£nh
                                if priors.max() > 0:
                                    priors = priors / (priors.max() + 1e-9)

                                # L∆∞u ƒë·∫ßy ƒë·ªß th√¥ng tin
                                np.savez_compressed(dst_file, 
                                    features=f_feats.astype(np.float32),
                                    boxes=np.array(valid_boxes, dtype=np.float32),
                                    confidences=valid_probs,
                                    areas=valid_areas,
                                    priors=priors
                                )
                                continue

                    # Tr∆∞·ªùng h·ª£p kh√¥ng c√≥ m·∫∑t n√†o th·ªèa m√£n
                    np.savez_compressed(dst_file, 
                        features=np.zeros((0, 2048), dtype=np.float32),
                        boxes=np.zeros((0, 4), dtype=np.float32),
                        confidences=np.zeros(0, dtype=np.float32),
                        areas=np.zeros(0, dtype=np.float32),
                        priors=np.zeros(0, dtype=np.float32)
                    )

                except Exception as e:
                    print(f"‚ùå L·ªói ·∫£nh {base}: {e}")
                    continue

    # ==========================================
    # 4. N√âN FILE SAU KHI HO√ÄN TH√ÄNH
    # ==========================================
    print("\n‚è≥ ƒêang n√©n file k·∫øt qu·∫£...")
    zip_path = '/kaggle/working/faces_priors_final'
    shutil.make_archive(zip_path, 'zip', CONFIG['output_dir'])
    print(f"‚úÖ HO√ÄN T·∫§T! File n√©n t·∫°i: {zip_path}.zip")

if __name__ == "__main__":
    run_extraction_full_priors()