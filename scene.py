import os
import torch
import torch.nn as nn
import numpy as np
from PIL import Image
import glob
from tqdm.auto import tqdm
from torchvision import transforms
import timm  # ƒê·∫£m b·∫£o ƒë√£ c√†i: pip install timm
import shutil
import warnings

warnings.filterwarnings('ignore')

# 1. C·∫•u h√¨nh
CONFIG = {
    'data_root': '/kaggle/input/mydata', 
    'scene_weights': '/kaggle/input/resnet50-scene-combined/resnet50_scene_combined.pth', 
    'output_dir': '/kaggle/working/features_scene_seresnet',
    'zip_name': '/kaggle/working/SiteGroEmo_scene_features',
    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')
}

os.makedirs(CONFIG['output_dir'], exist_ok=True)

# 2. H√†m kh·ªüi t·∫°o Model SE-ResNet50 chu·∫©n b√†i b√°o
def get_congnn_scene_extractor():
    print(f"üöÄ Kh·ªüi t·∫°o SE-ResNet50 (Chu·∫©n ConGNN)...")
    
    # B∆∞·ªõc A: Kh·ªüi t·∫°o model SE-ResNet50 v·ªõi 3 l·ªõp ƒë·∫ßu ra (nh∆∞ l√∫c b·∫°n fine-tune)
    # D√πng seresnet50 t·ª´ timm ƒë·ªÉ c√≥ c∆° ch·∫ø Squeeze-and-Excitation
    model = timm.create_model('seresnet50', pretrained=False, num_classes=3)
    
    # B∆∞·ªõc B: Load weights c·ªßa b·∫°n
    if os.path.exists(CONFIG['scene_weights']):
        print(f"‚úÖ ƒêang n·∫°p weights t·ª´: {CONFIG['scene_weights']}")
        checkpoint = torch.load(CONFIG['scene_weights'], map_location=CONFIG['device'])
        
        # X·ª≠ l√Ω n·∫øu checkpoint ch·ª©a 'state_dict'
        state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint
        
        try:
            model.load_state_dict(state_dict)
            print("  ‚ú® N·∫°p weights th√†nh c√¥ng!")
        except RuntimeError as e:
            print(f"  ‚ö†Ô∏è C·∫£nh b√°o: Ki·∫øn tr√∫c weights c√≥ th·ªÉ kh√°c SE-ResNet. ƒêang th·ª≠ n·∫°p m·ªÅm (strict=False)...")
            model.load_state_dict(state_dict, strict=False)
    else:
        print("  ‚ùå Kh√¥ng t√¨m th·∫•y weights. S·ª≠ d·ª•ng ImageNet m·∫∑c ƒë·ªãnh.")
        model = timm.create_model('seresnet50', pretrained=True, num_classes=3)

    # B∆∞·ªõc C: Chuy·ªÉn th√†nh Extractor (L·∫•y 2048 ƒë·∫∑c tr∆∞ng)
    # num_classes=0 trong timm s·∫Ω t·ª± ƒë·ªông x√≥a l·ªõp FC v√† tr·∫£ v·ªÅ feature vector
    model.reset_classifier(num_classes=0) 
    model = model.to(CONFIG['device'])
    model.eval()
    return model

# 3. Th·ª±c thi tr√≠ch xu·∫•t
def run_scene_extraction():
    extractor = get_congnn_scene_extractor()
    
    # Transform chu·∫©n c·ªßa SE-ResNet
    scene_tf = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    splits = ['train', 'val', 'test']
    emotions = ['Negative', 'Neutral', 'Positive']
    
    for split in splits:
        # T√¨m ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø (h·ªó tr·ª£ c·∫•u tr√∫c th∆∞ m·ª•c l·ªìng nhau)
        split_path = None
        for root, dirs, files in os.walk(CONFIG['data_root']):
            if split.lower() == os.path.basename(root).lower():
                split_path = root
                break
        
        if not split_path: continue
            
        print(f"\nüìÇ ƒêang x·ª≠ l√Ω t·∫≠p {split.upper()}...")
        out_root = os.path.join(CONFIG['output_dir'], 'scenes', split)
        
        for emotion in emotions:
            emo_dir = next((os.path.join(split_path, d) for d in os.listdir(split_path) 
                          if d.lower() == emotion.lower()), None)
            if not emo_dir: continue
            
            os.makedirs(os.path.join(out_root, emotion), exist_ok=True)
            img_files = glob.glob(os.path.join(emo_dir, '*'))
            img_files = [f for f in img_files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            
            for img_path in tqdm(img_files, desc=f" {emotion}", leave=False):
                base = os.path.splitext(os.path.basename(img_path))[0]
                dst_path = os.path.join(out_root, emotion, base + '.npy')
                
                if os.path.exists(dst_path): continue
                
                try:
                    img = Image.open(img_path).convert('RGB')
                    img_tensor = scene_tf(img).unsqueeze(0).to(CONFIG['device'])
                    
                    with torch.no_grad():
                        feat = extractor(img_tensor)
                        # feat l√∫c n√†y ƒë√£ l√† (1, 2048) nh·ªù model.reset_classifier(0)
                        feat_np = feat.cpu().numpy()[0]
                    
                    np.save(dst_path, feat_np)
                except Exception: continue

    print("\nüì¶ ƒêang n√©n k·∫øt qu·∫£...")
    shutil.make_archive(CONFIG['zip_name'], 'zip', CONFIG['output_dir'])
    print(f"‚úÖ Ho√†n t·∫•t! File l∆∞u t·∫°i: {CONFIG['zip_name']}.zip")

if __name__ == "__main__":
    run_scene_extraction()